# 文本切块策略分析报告

## 任务概述
- **目标**：分析 haiku.rag 项目的文本切块策略
- **需求来源**：用户询问"该项目目前的切块是怎么切的？"

## 修改范围与文件变动
- 无修改，仅分析现有代码
- 主要分析文件：`src/haiku/rag/chunker.py`

## 切块策略详细分析

### 1. 核心技术选型
- **Tokenizer**：tiktoken（OpenAI 的 tokenization 库）
- **编码模型**：gpt-4o（最新的编码方式）
- **实现方式**：基于 token 的智能切分

### 2. 默认配置参数
```python
CHUNK_SIZE = 1024      # 每个块的最大 token 数
CHUNK_OVERLAP = 256    # 块之间的重叠 token 数
```

### 3. 文本预处理流程

#### 3.1 空白字符规范化
```python
text = re.sub(r'\s+', ' ', text)  # 多个空白符替换为单个空格
```

#### 3.2 句子边界标记
- **中文句子结束符**：。！？；
- **英文句子结束符**：. ! ? ;（后跟空格）
- 在句子结束符后自动添加换行符，便于后续分割

#### 3.3 段落处理
```python
text = re.sub(r'\n\s*\n', '\n\n', text)  # 规范化段落分隔
```

### 4. 智能分割算法

#### 4.1 评分机制
系统为不同的分割点类型分配权重分数：
```python
句子结束符（。！？；. ! ? ;）: 10分
段落分隔符（\n\n）: 8分
换行符（\n）: 5分
逗号（，,）: 2分
```

#### 4.2 搜索策略
- 搜索窗口：目标位置前后 100 个字符（或文本长度的 25%，取较小值）
- 距离惩罚：离目标位置越远，分数越低
- 最终分数 = 基础分数 × (1 - 距离惩罚)

#### 4.3 分割流程
1. 将文本编码为 tokens
2. 按照 chunk_size 确定初步分割位置
3. 在分割位置附近寻找最佳分割点
4. 解码并重新编码以获得精确的 token 边界
5. 保留 chunk_overlap 个 tokens 作为下一块的开始

### 5. 实际案例演示

假设有以下中文文本：
```
人工智能技术正在快速发展。机器学习是其中的重要分支，深度学习更是近年来的热点。
通过神经网络，计算机可以学习复杂的模式。这带来了许多应用机会。
```

切块过程：
1. **预处理后**：
   ```
   人工智能技术正在快速发展。\n机器学习是其中的重要分支，深度学习更是近年来的热点。\n
   通过神经网络，计算机可以学习复杂的模式。\n这带来了许多应用机会。\n
   ```

2. **Token 编码**：假设总共 150 tokens，chunk_size=100
   - 第一次切分目标：100 tokens
   - 寻找最近的句号（。）作为分割点
   - 实际切分可能在 95 tokens 处（句子结束）

3. **生成块**：
   - 块1：[0-95 tokens] "人工智能技术正在快速发展。机器学习是其中的重要分支，深度学习更是近年来的热点。"
   - 块2：[70-150 tokens] "深度学习更是近年来的热点。通过神经网络，计算机可以学习复杂的模式。这带来了许多应用机会。"
   - 注意：有 25 tokens 的重叠

### 6. 优势与特点

#### 6.1 语义完整性
- 避免在句子中间切断，保持语义单元完整
- 特别适合问答系统，确保检索到的内容可读性高

#### 6.2 多语言优化
- 同时支持中英文标点识别
- 对中文文档特别友好（识别中文标点）

#### 6.3 灵活性
- chunk_size 和 overlap 可配置
- 可根据不同场景调整参数

#### 6.4 上下文保持
- 256 tokens 的重叠确保：
  - 相关信息不会因切分而丢失
  - 检索时有足够的上下文理解

### 7. 性能考虑

- **时间复杂度**：O(n)，其中 n 为文本长度
- **空间复杂度**：O(n)，需要存储编码后的 tokens
- **性能优化**：使用类变量存储编码器，避免重复初始化

### 8. 潜在改进建议

1. **自适应块大小**：根据文档类型动态调整 chunk_size
2. **语义相似度检测**：在分割时考虑语义连贯性
3. **并行处理**：对长文档使用并行切块
4. **缓存机制**：缓存常见文本模式的切块结果

## 问题记录与解决方法
无

## 后续建议

1. **监控切块质量**：建立切块效果评估指标
2. **A/B 测试**：比较不同 chunk_size 和 overlap 的检索效果
3. **文档类型优化**：针对代码、文档、对话等不同类型优化切块策略
4. **性能优化**：对于大规模文档处理，考虑批量化和并行化

## 总结

haiku.rag 的切块策略采用了基于 token 的智能切分方法，结合了精确的长度控制和语义完整性保护。通过评分机制选择最佳分割点，确保生成的文本块既满足大小要求，又保持良好的可读性。这种方法特别适合 RAG 系统的需求，能够提供高质量的检索结果。