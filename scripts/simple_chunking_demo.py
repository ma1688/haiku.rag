#!/usr/bin/env python3
"""
ç®€åŒ–çš„æ–‡æœ¬åˆ‡ç‰‡æ¼”ç¤ºè„šæœ¬

è¿™ä¸ªè„šæœ¬ä¸“æ³¨äºå±•ç¤ºæ–‡æœ¬åˆ‡ç‰‡åŠŸèƒ½ï¼Œä¸éœ€è¦é…ç½®åµŒå…¥æ¨¡å‹ã€‚
å±•ç¤ºå†…å®¹ï¼š
1. åŸå§‹æ–‡æœ¬
2. åˆ‡ç‰‡åçš„æ–‡æœ¬å—
3. æ¯ä¸ªå—çš„è¯¦ç»†ä¿¡æ¯
4. åˆ‡ç‰‡ç»Ÿè®¡åˆ†æ
"""

import asyncio
import sys
from pathlib import Path
from typing import List, Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

from haiku.rag.chunker import Chunker


def print_header(title: str, char: str = "=", width: int = 80):
    """æ‰“å°æ ‡é¢˜"""
    print(f"\n{char * width}")
    print(f"{title:^{width}}")
    print(f"{char * width}")


def print_section(title: str, char: str = "-", width: int = 60):
    """æ‰“å°ç« èŠ‚æ ‡é¢˜"""
    print(f"\n{char * width}")
    print(f" {title}")
    print(f"{char * width}")


async def demo_chunking(text: str, chunk_size: int = 512, chunk_overlap: int = 64):
    """æ¼”ç¤ºæ–‡æœ¬åˆ‡ç‰‡åŠŸèƒ½"""
    
    print_header("æ–‡æœ¬åˆ‡ç‰‡æ¼”ç¤º")
    
    # åˆ›å»ºåˆ‡ç‰‡å™¨
    chunker = Chunker(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    
    # 1. æ˜¾ç¤ºåŸå§‹æ–‡æœ¬ä¿¡æ¯
    print_section("1. åŸå§‹æ–‡æœ¬åˆ†æ")
    original_tokens = chunker.encoder.encode(text, disallowed_special=())
    
    print(f"æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
    print(f"Tokenæ•°é‡: {len(original_tokens)} tokens")
    print(f"åˆ‡ç‰‡é…ç½®: {chunk_size} tokens/å—, {chunk_overlap} tokensé‡å ")
    
    print(f"\nåŸå§‹æ–‡æœ¬å†…å®¹:")
    print("-" * 40)
    print(text[:300] + "..." if len(text) > 300 else text)
    print("-" * 40)
    
    # 2. æ‰§è¡Œåˆ‡ç‰‡
    print_section("2. æ–‡æœ¬åˆ‡ç‰‡è¿‡ç¨‹")
    chunks = await chunker.chunk(text)
    print(f"åˆ‡ç‰‡å®Œæˆï¼Œå…±ç”Ÿæˆ {len(chunks)} ä¸ªæ–‡æœ¬å—")
    
    # 3. åˆ†ææ¯ä¸ªåˆ‡ç‰‡
    print_section("3. åˆ‡ç‰‡è¯¦ç»†åˆ†æ")
    
    total_chunk_chars = 0
    total_chunk_tokens = 0
    
    for i, chunk in enumerate(chunks):
        chunk_tokens = chunker.encoder.encode(chunk, disallowed_special=())
        chunk_char_len = len(chunk)
        chunk_token_len = len(chunk_tokens)
        
        total_chunk_chars += chunk_char_len
        total_chunk_tokens += chunk_token_len
        
        print(f"\nğŸ“„ åˆ‡ç‰‡ {i+1}:")
        print(f"   å­—ç¬¦æ•°: {chunk_char_len}")
        print(f"   Tokenæ•°: {chunk_token_len}")
        print(f"   Tokenå¯†åº¦: {chunk_char_len/chunk_token_len:.2f} å­—ç¬¦/token")
        
        # æ˜¾ç¤ºåˆ‡ç‰‡å†…å®¹
        chunk_preview = chunk[:150].replace('\n', ' ').strip()
        if len(chunk) > 150:
            chunk_preview += "..."
        print(f"   å†…å®¹: {chunk_preview}")
        
        # æ˜¾ç¤ºä¸å‰ä¸€ä¸ªåˆ‡ç‰‡çš„é‡å æƒ…å†µ
        if i > 0:
            prev_chunk = chunks[i-1]
            # ç®€å•çš„é‡å æ£€æµ‹ï¼šæ£€æŸ¥å½“å‰å—çš„å¼€å¤´æ˜¯å¦åœ¨å‰ä¸€ä¸ªå—ä¸­å‡ºç°
            overlap_chars = 0
            chunk_start = chunk[:100]  # å–å½“å‰å—çš„å‰100å­—ç¬¦
            if chunk_start in prev_chunk:
                overlap_start = prev_chunk.find(chunk_start)
                if overlap_start != -1:
                    overlap_chars = len(prev_chunk) - overlap_start
            
            if overlap_chars > 0:
                print(f"   ä¸å‰å—é‡å : ~{overlap_chars} å­—ç¬¦")
    
    # 4. ç»Ÿè®¡æ‘˜è¦
    print_section("4. åˆ‡ç‰‡ç»Ÿè®¡æ‘˜è¦")
    
    print(f"ğŸ“Š åŸå§‹æ–‡æœ¬ç»Ÿè®¡:")
    print(f"   æ€»å­—ç¬¦æ•°: {len(text)}")
    print(f"   æ€»Tokenæ•°: {len(original_tokens)}")
    
    print(f"\nğŸ“Š åˆ‡ç‰‡åç»Ÿè®¡:")
    print(f"   åˆ‡ç‰‡æ•°é‡: {len(chunks)}")
    print(f"   æ€»å­—ç¬¦æ•°: {total_chunk_chars}")
    print(f"   æ€»Tokenæ•°: {total_chunk_tokens}")
    
    # è®¡ç®—é‡å¤ç‡
    char_duplication = (total_chunk_chars - len(text)) / len(text) * 100
    token_duplication = (total_chunk_tokens - len(original_tokens)) / len(original_tokens) * 100
    
    print(f"\nğŸ“Š é‡å åˆ†æ:")
    print(f"   å­—ç¬¦é‡å¤ç‡: {char_duplication:.2f}%")
    print(f"   Tokené‡å¤ç‡: {token_duplication:.2f}%")
    
    # åˆ‡ç‰‡å¤§å°åˆ†å¸ƒ
    chunk_sizes = [len(chunker.encoder.encode(chunk, disallowed_special=())) for chunk in chunks]
    print(f"\nğŸ“Š åˆ‡ç‰‡å¤§å°åˆ†å¸ƒ:")
    print(f"   æœ€å°Tokenæ•°: {min(chunk_sizes)}")
    print(f"   æœ€å¤§Tokenæ•°: {max(chunk_sizes)}")
    print(f"   å¹³å‡Tokenæ•°: {sum(chunk_sizes)/len(chunk_sizes):.1f}")
    print(f"   ç›®æ ‡Tokenæ•°: {chunk_size}")
    
    # 5. åˆ‡ç‰‡è´¨é‡è¯„ä¼°
    print_section("5. åˆ‡ç‰‡è´¨é‡è¯„ä¼°")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è¿‡å°çš„åˆ‡ç‰‡
    small_chunks = [i for i, size in enumerate(chunk_sizes) if size < chunk_size * 0.5]
    if small_chunks:
        print(f"âš ï¸  å‘ç° {len(small_chunks)} ä¸ªè¾ƒå°çš„åˆ‡ç‰‡ (< 50%ç›®æ ‡å¤§å°): {[i+1 for i in small_chunks]}")
    else:
        print("âœ… æ‰€æœ‰åˆ‡ç‰‡å¤§å°éƒ½åœ¨åˆç†èŒƒå›´å†…")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è¿‡å¤§çš„åˆ‡ç‰‡
    large_chunks = [i for i, size in enumerate(chunk_sizes) if size > chunk_size * 1.1]
    if large_chunks:
        print(f"âš ï¸  å‘ç° {len(large_chunks)} ä¸ªè¾ƒå¤§çš„åˆ‡ç‰‡ (> 110%ç›®æ ‡å¤§å°): {[i+1 for i in large_chunks]}")
    else:
        print("âœ… æ‰€æœ‰åˆ‡ç‰‡å¤§å°éƒ½åœ¨ç›®æ ‡èŒƒå›´å†…")
    
    # è¯„ä¼°åˆ‡ç‰‡è¾¹ç•Œè´¨é‡
    boundary_quality = 0
    for chunk in chunks[:-1]:  # é™¤äº†æœ€åä¸€ä¸ªåˆ‡ç‰‡
        # æ£€æŸ¥æ˜¯å¦åœ¨å¥å­è¾¹ç•Œç»“æŸ
        if chunk.rstrip().endswith(('.', 'ã€‚', '!', 'ï¼', '?', 'ï¼Ÿ')):
            boundary_quality += 1
        # æ£€æŸ¥æ˜¯å¦åœ¨æ®µè½è¾¹ç•Œç»“æŸ
        elif chunk.rstrip().endswith('\n'):
            boundary_quality += 0.8
        # æ£€æŸ¥æ˜¯å¦åœ¨é€—å·æˆ–åˆ†å·ç»“æŸ
        elif chunk.rstrip().endswith((',', 'ï¼Œ', ';', 'ï¼›')):
            boundary_quality += 0.5
    
    boundary_score = boundary_quality / (len(chunks) - 1) * 100 if len(chunks) > 1 else 100
    print(f"ğŸ“ˆ åˆ‡ç‰‡è¾¹ç•Œè´¨é‡è¯„åˆ†: {boundary_score:.1f}%")
    
    return chunks


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ç®€åŒ–æ–‡æœ¬åˆ‡ç‰‡æ¼”ç¤º")
    
    # ç¤ºä¾‹æ–‡æœ¬ - æ··åˆä¸­è‹±æ–‡å†…å®¹
    sample_text = """
    è³ä¹‹å‘³æ§è‚¡æœ‰é™å…¬å¸ï¼ˆã€Œæœ¬å…¬å¸ã€ï¼‰è‘£äº‹æœƒï¼ˆã€Œè‘£äº‹æœƒã€ï¼‰è¬¹æ­¤å®£ä½ˆï¼Œæœ¬å…¬å¸å°‡æ–¼2024å¹´9æœˆ19æ—¥ï¼ˆæ˜ŸæœŸå››ï¼‰ä¸‹åˆ2æ™‚30åˆ†å‡åº§é¦™æ¸¯ä¹é¾è§€å¡˜é–‹æºé“79è™Ÿé±·é­šæ¤ä¸­å¿ƒ6æ¨“èˆ‰è¡Œè‚¡æ±é€±å¹´å¤§æœƒï¼ˆã€Œè‚¡æ±å¤§æœƒã€ï¼‰ã€‚

    è‚¡æ±å¤§æœƒå°‡å¯©è­°ä»¥ä¸‹äº‹é …ï¼š
    1. çœè¦½åŠæ¡ç´æˆªè‡³2024å¹´3æœˆ31æ—¥æ­¢å¹´åº¦ä¹‹ç¶“å¯©æ ¸è²¡å‹™å ±è¡¨åŠè‘£äº‹æœƒå ±å‘Šæ›¸å’Œæ ¸æ•¸å¸«å ±å‘Šæ›¸ï¼›
    2. é‡é¸é€€ä»»è‘£äº‹ï¼›
    3. çºŒè˜æ ¸æ•¸å¸«åŠæˆæ¬Šè‘£äº‹æœƒé‡å®šå…¶é…¬é‡‘ï¼›
    4. è€ƒæ…®åŠé…Œæƒ…é€šéæˆå‡ºè³¼å›è‚¡ä»½ä¹‹ä¸€èˆ¬æˆæ¬Šï¼›
    5. è€ƒæ…®åŠé…Œæƒ…é€šéæˆå‡ºç™¼è¡Œè‚¡ä»½ä¹‹ä¸€èˆ¬æˆæ¬Šã€‚

    æ‰€æœ‰è‚¡æ±å‡ç²é‚€è«‹å‡ºå¸­è‚¡æ±å¤§æœƒã€‚æœªèƒ½è¦ªèº«å‡ºå¸­å¤§æœƒä¹‹è‚¡æ±å¯å§”ä»»ä»£è¡¨ä»£ç‚ºå‡ºå¸­åŠæŠ•ç¥¨ã€‚ä»£è¡¨å§”ä»»è¡¨æ ¼é€£åŒç°½ç½²æˆæ¬Šæ›¸æˆ–å…¶ä»–æˆæ¬Šæ–‡ä»¶ï¼ˆå¦‚æœ‰ï¼‰é ˆæ–¼è‚¡æ±å¤§æœƒèˆ‰è¡Œå‰ä¸å°‘æ–¼48å°æ™‚é€é”æœ¬å…¬å¸ä¹‹è‚¡ä»½éæˆ¶ç™»è¨˜è™•å“ä½³è­‰åˆ¸ç™»è¨˜æœ‰é™å…¬å¸ï¼Œåœ°å€ç‚ºé¦™æ¸¯çš‡åå¤§é“æ±183è™Ÿåˆå’Œä¸­å¿ƒ54æ¨“ã€‚

    Financial Summary for the Year Ended March 31, 2024:
    - Revenue: HK$1,234,567,890
    - Gross Profit: HK$234,567,890  
    - Net Profit: HK$123,456,789
    - Earnings Per Share: HK$0.12
    - Dividend Per Share: HK$0.05

    Business Outlook:
    The Company will continue to focus on core business development, enhancing competitiveness through product innovation and market expansion. Management expects to face market challenges in the coming year but remains optimistic about long-term development prospects.

    æœ¬å…¬å¸å°‡ç¹¼çºŒå°ˆæ³¨æ–¼æ ¸å¿ƒæ¥­å‹™ç™¼å±•ï¼Œé€šéç”¢å“å‰µæ–°å’Œå¸‚å ´æ‹“å±•ä¾†æå‡ç«¶çˆ­åŠ›ã€‚é æœŸæœªä¾†ä¸€å¹´å°‡é¢è‡¨å¸‚å ´æŒ‘æˆ°ï¼Œä½†ç®¡ç†å±¤å°é•·æœŸç™¼å±•å‰æ™¯ä¿æŒæ¨‚è§€æ…‹åº¦ã€‚æˆ‘å€‘è¨ˆåŠƒåœ¨ä»¥ä¸‹å¹¾å€‹æ–¹é¢åŠ å¼·æŠ•å…¥ï¼š

    1. æŠ€è¡“ç ”ç™¼ï¼šå¢åŠ ç ”ç™¼æŠ•å…¥ï¼Œæå‡ç”¢å“æŠ€è¡“å«é‡
    2. å¸‚å ´ç‡ŸéŠ·ï¼šæ“´å¤§å“ç‰Œå½±éŸ¿åŠ›ï¼Œé–‹æ‹“æ–°çš„å¸‚å ´é ˜åŸŸ
    3. äººæ‰åŸ¹é¤Šï¼šå»ºç«‹å®Œå–„çš„äººæ‰åŸ¹é¤Šé«”ç³»ï¼Œæå‡åœ˜éšŠæ•´é«”ç´ è³ª
    4. ä¾›æ‡‰éˆå„ªåŒ–ï¼šå„ªåŒ–ä¾›æ‡‰éˆç®¡ç†ï¼Œé™ä½é‹ç‡Ÿæˆæœ¬
    5. æ•¸å­—åŒ–è½‰å‹ï¼šæ¨é€²ä¼æ¥­æ•¸å­—åŒ–è½‰å‹ï¼Œæé«˜é‹ç‡Ÿæ•ˆç‡
    """
    
    try:
        # ä½¿ç”¨ä¸åŒçš„åˆ‡ç‰‡é…ç½®é€²è¡Œæ¼”ç¤º
        await demo_chunking(sample_text.strip(), chunk_size=256, chunk_overlap=32)
        
        print_header("æ¼”ç¤ºå®Œæˆ", char="*")
        print("ğŸ’¡ æç¤ºï¼šæ‚¨å¯ä»¥ä¿®æ”¹ chunk_size å’Œ chunk_overlap åƒæ•¸ä¾†æ¸¬è©¦ä¸åŒçš„åˆ‡ç‰‡æ•ˆæœ")
        print("ğŸ’¡ å»ºè­°ï¼šå°æ–¼ä¸­æ–‡æ–‡æœ¬ï¼Œå¯ä»¥ä½¿ç”¨è¼ƒå¤§çš„ chunk_size (å¦‚ 512-1024)")
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºéç¨‹ä¸­å‡ºç¾éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
